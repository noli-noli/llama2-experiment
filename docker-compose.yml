version: "3"
services:
  gp2xaxxx:
    #コンテナの名前を指定
    container_name: llama2-experiment

    #ビルド
    build:
      context: .
      #ビルドする際に使用するdockerfileを指定
      dockerfile: dockerfile

      #dockerfileで使用する変数を渡す。
      args:
        - http_tmp=$http_proxy
        - https_tmp=$https_proxy

    #ワーキングディレクトリを/workspaceに設定
    volumes:
      - ./workspace:/workspace
      - /srv/models:/workspace/models

    #ワーキングディレクトリを/workspaceに設定
    working_dir: /workspace

    #コンテナ起動時に実行するプログラムを記述
    command: /sbin/init

    #学内用proxyを含めた環境変数ファイルを読込む
    env_file:
      - proxy.env

    #ttyとprivilegedを有効化
    privileged: true
    tty: true

    #キーボード割込みによるコンテナの停止時、コンテナ内のプロセスに正常終了を通知する。
    stop_signal: SIGINT

    #Portの設定
    #ports:
    #  - "8080:8080" #フロントエンド
    #  - "49152:49152" #バックエンド

    #GPUを使用するための設定
    deploy:
      resources:
        #コンテナが使用するRAMの上限を設定する際は以下2行をコメントアウト
        #limits:
        #  memory: 24G
        reservations:
          devices:
            - capabilities: [gpu]

    #core dumpを防ぐ為の設定
    shm_size: 12GB
